{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "if \"/nas/home/minhpham/workspace/kb-data-cleaning/\" not in sys.path:\n",
    "    sys.path.insert(0, \"/nas/home/minhpham/workspace/kb-data-cleaning/\")\n",
    "\n",
    "os.chdir(\"../../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "name2raw = {}\n",
    "name2cleaned = {}\n",
    "name2groundtruth = {}\n",
    "\n",
    "data_path = Path(\"data/test/ed2/\")\n",
    "\n",
    "for file_path in (data_path / \"raw\").iterdir():\n",
    "    name2raw[file_path.name] = pd.read_csv(file_path, keep_default_na=False, dtype=str)\n",
    "    name2cleaned[file_path.name] = pd.read_csv(\n",
    "        data_path / \"cleaned\" / file_path.name, keep_default_na=False, dtype=str\n",
    "    )\n",
    "    name2groundtruth[file_path.name] = (\n",
    "        name2raw[file_path.name] == name2cleaned[file_path.name]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kbclean.cleaning.detection.deep import DeepUnDetector\n",
    "from kbclean.utils.inout import load_config\n",
    "\n",
    "configs = load_config(\"config\")\n",
    "\n",
    "deep_detector = DeepUnDetector(configs.deep_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "# from nltk.util import trigrams\n",
    "\n",
    "def _to_regex(x):\n",
    "    try:\n",
    "        if x is None:\n",
    "            return \"\"\n",
    "        x = re.sub(r\"[A-Z]\", \"A\", x)\n",
    "        x = re.sub(r\"[0-9]\", \"0\", x)\n",
    "        x = re.sub(r\"[a-z]\", \"a\", x)\n",
    "        return x\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return x\n",
    "\n",
    "\n",
    "# def ngram_featurize(str_):\n",
    "#     feature_dict = {}\n",
    "#     if len(str_) < 3:\n",
    "#         str_ += \"|\" * (3 - len(str_))\n",
    "#     for trigram in trigrams(str_):\n",
    "#         feature_dict[f\"{''.join(trigram)}\"] = 1\n",
    "\n",
    "#     for trigram in trigrams(_to_regex(str_)):\n",
    "#         feature_dict[f\"pattern_{''.join(trigram)}\"] = 1\n",
    "\n",
    "#     return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "def detect_values(raw_data, groundtruth):\n",
    "#     vectorizer = DictVectorizer()\n",
    "#     feature_dicts = [ngram_featurize(val) for val in raw_data]\n",
    "#     X = vectorizer.fit_transform(feature_dicts)\n",
    "    patterns = list(map(_to_regex, raw_data))\n",
    "    tensors = []\n",
    "\n",
    "    for i in range(0, len(patterns), deep_detector.hparams.batch_size):\n",
    "        tensor = deep_detector.lm_model.encode(patterns[i : i + deep_detector.hparams.batch_size])\n",
    "        tensors.append(tensor)\n",
    "    probs = torch.cat(tensors, dim=0).detach().cpu().numpy()\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        probs, groundtruth, test_size=0.5, random_state=42\n",
    "    )\n",
    "\n",
    "    random_forest = RandomForestClassifier(n_jobs=64)\n",
    "    random_forest.fit(X_train, y_train)\n",
    "\n",
    "    y_predict = random_forest.predict(X_test)\n",
    "    return y_predict, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(raw_data, groundtruth):\n",
    "    combined_y_test = []\n",
    "    combined_y_predict = []\n",
    "    for column in raw_data.columns:\n",
    "        y_predict, y_test = detect_values(\n",
    "            raw_data[column].values.tolist(), groundtruth[column].values.tolist()\n",
    "        )\n",
    "        combined_y_predict.extend(y_predict)\n",
    "        combined_y_test.extend(y_test)\n",
    "    return combined_y_test, combined_y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "name2report = {}\n",
    "\n",
    "for name, raw_data in name2raw.items():\n",
    "    ground_truth = name2groundtruth[name]\n",
    "    combined_y_test, combined_y_predict = detect(raw_data, ground_truth)\n",
    "    name2report[name] = pd.DataFrame(\n",
    "        classification_report(combined_y_test, combined_y_predict, output_dict=True)\n",
    "    ).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from labext.prelude import A, M, W\n",
    "\n",
    "M.DataTable.register()\n",
    "\n",
    "def render(index):\n",
    "    item = list(name2report.items())[index]\n",
    "    display(item[0], item[1])\n",
    "\n",
    "\n",
    "A.slider(render, max=len(name2report.values()) - 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('torch': conda)",
   "language": "python",
   "name": "python37764bittorchconda619657fd9e3249e097c69e9d63018998"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
