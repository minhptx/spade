{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Detection - Training LSTM model (char + word embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up environment (Pytorch + Pandas + Numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "\n",
    "if r\"../../../kb-data-cleaning/kbclean\" not in sys.path:\n",
    "    sys.path.append(r\"../../../kb-data-cleaning/kbclean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config import load_hparams\n",
    "\n",
    "hparams = load_hparams(\"../../config/hparams.yaml\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data using TorchText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "def preprocess(str1):\n",
    "    str1 = \"\".join(str1)\n",
    "    str1 = re.sub(\"[A-Z]\", \"A\", str1)\n",
    "    str1 = re.sub(\"[a-z]\", \"a\", str1)\n",
    "    str1 = re.sub(\"[0-9]\", \"0\", str1)\n",
    "\n",
    "    return list(str1) \n",
    "\n",
    "type_to_regex = {\"UPPERCASE\": \"[A-Z]+\", \"LOWERCASE\": \"[a-z]+\", \"DIGIT\": \"[0-9]+\", \"ALPHABET\": \"[A-Za-z]+\",\"ALPHANUM\": \"[A-Za-z0-9]+\"}\n",
    "\n",
    "def mask_word(tokens):\n",
    "    masked_tokens = []\n",
    "    for token in tokens:\n",
    "        for type_, regex in type_to_regex.items():\n",
    "            if re.match(f\"^{regex}$\", token):\n",
    "                masked_tokens.append(type_)\n",
    "                break\n",
    "        else:\n",
    "            masked_tokens.append(token)\n",
    "    return masked_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import (\n",
    "    TabularDataset,\n",
    "    Field,\n",
    "    NestedField,\n",
    "    LabelField,\n",
    ")\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "nesting_field = Field(\n",
    "    tokenize=list,\n",
    "    pad_token=\"<cpad>\",\n",
    "    init_token=\"<w>\",\n",
    "    eos_token=\"</w>\",\n",
    "    batch_first=True,\n",
    "    fix_length=hparams.max_char_length,\n",
    "    preprocessing=preprocess\n",
    ")\n",
    "\n",
    "char1w_field = NestedField(\n",
    "    nesting_field,\n",
    "    pad_token=\"<wpad>\",\n",
    "    include_lengths=True\n",
    ")\n",
    "\n",
    "word_field = Field(\n",
    "    pad_token=\"<wpad>\",\n",
    "    batch_first=True,\n",
    "    lower=False,\n",
    "    include_lengths=True,\n",
    ")\n",
    "\n",
    "label = LabelField()\n",
    "\n",
    "dataset = TabularDataset(\n",
    "    path=\"../../data/train/train_500000c.csv\",\n",
    "    format=\"csv\",\n",
    "    fields={\n",
    "        \"str1\": [(\"src_word\", word_field), (\"src_char\", char1w_field)],\n",
    "        \"str2\": [(\"trg_word\", word_field), (\"trg_char\", char1w_field)],\n",
    "        \"sim\": [(\"lbl\", label)],\n",
    "    },\n",
    ")\n",
    "\n",
    "test_dataset = TabularDataset(\n",
    "    path=\"../../data/test/wiki/labeled_output.csv\",\n",
    "    format=\"csv\",\n",
    "    fields={\n",
    "        \"str1\": [(\"src_word\", word_field), (\"src_char\", char1w_field)],\n",
    "        \"str2\": [(\"trg_word\", word_field), (\"trg_char\", char1w_field)],\n",
    "        \"sim\": [(\"lbl\", label)]\n",
    "    },\n",
    "    csv_reader_params={\n",
    "        \"quotechar\":\"'\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building language vocabulary from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126, 33014)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext import vocab\n",
    "from pathlib import Path\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "word_field.build_vocab(dataset.src_word, dataset.trg_word)\n",
    "char1w_field.build_vocab(dataset.src_char, dataset.trg_char)\n",
    "\n",
    "label.build_vocab(dataset.lbl)\n",
    "label.vocab.stoi = {\"True\": 0, \"False\": 1}\n",
    "\n",
    "hparams.word_vocab_size = len(word_field.vocab)\n",
    "hparams.char_vocab_size = len(char1w_field.vocab)\n",
    "\n",
    "hparams.char_vocab_size, hparams.word_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import BucketIterator\n",
    "\n",
    "train_iterator = BucketIterator(dataset, device=device, batch_size=hparams.batch_size)\n",
    "test_iterator = BucketIterator(test_dataset, device=device, batch_size=hparams.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing callback function for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0: 12C   --- 10C    ==>  --- 0\n",
      "Example 1: L, 51-7  --- Oct 6   ==>  --- 1\n",
      "Example 2: blocker   --- refactor    ==>  --- 1\n",
      "Example 3: Apr 3, 2007 --- US7489094    ==>  --- 1\n",
      "Example 4: at Texas  --- Home    ==>  --- 1\n",
      "Example 5: .000   --- 16.6    ==>  --- 0\n",
      "Example 6: Fort Madison, Iowa --- virginia    ==>  --- 0\n",
      "Example 7: 24 Feb 2011 --- 18 Mar 2009  ==>  --- 0\n",
      "Example 8: 19 Mar 2009 --- 1 Feb 2013  ==>  --- 0\n",
      "Example 9: 12/10/2012 03:39AM  --- 12/10/2012 03:08AM   ==>  --- 0\n",
      "Example 10: comments 0  --- # times read 1 ==>  --- 1\n",
      "Example 11: Sioux City  --- 67°F    ==>  --- 1\n",
      "Example 12: -7.6 °F  --- West (280°)   ==>  --- 1\n",
      "Example 13: REMI   --- Mar 17, 2008  ==>  --- 1\n",
      "Example 14: 1   --- /tmp/apc/apc.9BtWBv    ==>  --- 0\n",
      "Example 15: Glg   --- NY    ==>  --- 0\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def idx2word(index_sequences, field):\n",
    "    return \" \".join(\n",
    "        [field.vocab.itos[idx] if idx != 1 else \"\" for idx in index_sequences]\n",
    "    )\n",
    "\n",
    "\n",
    "def show_samples(trainer):\n",
    "    i = 0\n",
    "    for batch in train_iterator:\n",
    "        x, y = batch\n",
    "#         y_hat = trainer(*x)\n",
    "        y = y.reshape(-1, 1)\n",
    "\n",
    "        for i in range(len(y)):\n",
    "            print(\n",
    "                f\"Example {i}: {idx2word(x[0][0][i], word_field)} ---\"\n",
    "                f\" {idx2word(x[2][0][i], word_field)} ==>  --- {y[i].item()}\"\n",
    "            )\n",
    "        break\n",
    "        \n",
    "show_samples(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and training LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "\n",
    "glove = vocab.GloVe(name=\"6B\", dim=300)\n",
    "\n",
    "# char_cnn = CharCNN1W(\n",
    "#     max_char_length=20,\n",
    "#     char_vocab_size=hparams.char_vocab_size,\n",
    "#     char_embedding_size=hparams.char_embedding_size,\n",
    "#     dropout=0.2,\n",
    "#     output_size=50,\n",
    "# )\n",
    "\n",
    "char_cnn = MultiCharCNN(\n",
    "    char_vocab_size=hparams.char_vocab_size,\n",
    "    char_embedding_size=hparams.char_embedding_size\n",
    ")\n",
    "\n",
    "lstm = CharCNNLSTM(\n",
    "    char_cnn,\n",
    "    word_vocab_size=hparams.word_vocab_size,\n",
    "    embedding_size=hparams.embedding_size,\n",
    "    hidden_size=hparams.hidden_size,\n",
    "    pretrained_embeddings=glove.vectors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Validation sanity check', layout=Layout(flex='2'), max=5.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0165d975dcfb42afaef1347345bb4b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max=1.0), HTML(value='')), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Validating', layout=Layout(flex='2'), max=22.0, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Validating', layout=Layout(flex='2'), max=22.0, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Validating', layout=Layout(flex='2'), max=22.0, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "trainer = Trainer(gpus=1, amp_level='O1', benchmark=False, default_save_path=\"../../checkpoints\", )\n",
    "trainer.fit(lstm, train_dataloader=train_iterator, val_dataloaders=[test_iterator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "0.15.0"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
