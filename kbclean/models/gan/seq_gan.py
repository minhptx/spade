import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from pytorch_lightning import LightningModule
from torch.autograd import Variable

from models.gan.rollout import Rollout


class Generator(nn.Module):
    """Generator """
    def __init__(self, num_emb, emb_dim, hidden_dim):
        super(Generator, self).__init__()
        self.num_emb = num_emb
        self.emb_dim = emb_dim
        self.hidden_dim = hidden_dim
        self.emb = nn.Embedding(num_emb, emb_dim)
        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)
        self.lin = nn.Linear(hidden_dim, num_emb)
        self.softmax = nn.LogSoftmax()
        self.init_params()

    def forward(self, x):
        """
        Args:
            x: (batch_size, seq_len), sequence of tokens generated by generator
        """
        emb = self.emb(x)
        h0, c0 = self.init_hidden(x.size(0))
        output, (h, c) = self.lstm(emb, (h0, c0))
        pred = self.softmax(
            self.lin(output.contiguous().view(-1, self.hidden_dim)))
        return pred

    def step(self, x, h, c):
        """
        Args:
            x: (batch_size,  1), sequence of tokens generated by generator
            h: (1, batch_size, hidden_dim), lstm hidden state
            c: (1, batch_size, hidden_dim), lstm cell state
        """
        emb = self.emb(x)
        output, (h, c) = self.lstm(emb, (h, c))
        pred = F.softmax(self.lin(output.view(-1, self.hidden_dim)), dim=1)
        return pred, h, c

    def init_hidden(self, batch_size):
        h = torch.zeros((1, batch_size, self.hidden_dim), device=self.device)
        c = torch.zeros((1, batch_size, self.hidden_dim), device=self.device)
        return h, c

    def init_params(self):
        for param in self.parameters():
            param.data.uniform_(-0.05, 0.05)

    def sample(self, batch_size, seq_len, x=None):
        flag = False  # whether sample from zero
        if x is None:
            flag = True
        if flag:
            x = torch.zeros((batch_size, 1), device=self.device).long()

        h, c = self.init_hidden(batch_size)
        samples = []
        if flag:
            for i in range(seq_len):
                output, h, c = self.step(x, h, c)
                x = output.multinomial(1)
                samples.append(x)
        else:
            given_len = x.size(1)
            lis = x.chunk(x.size(1), dim=1)
            for i in range(given_len):
                output, h, c = self.step(lis[i], h, c)
                samples.append(lis[i])
            x = output.multinomial(1)
            for i in range(given_len, seq_len):
                samples.append(x)
                output, h, c = self.step(x, h, c)
                x = output.multinomial(1)
        output = torch.cat(samples, dim=1)
        return output

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self.forward(x)
        loss = F.nll_loss(y_hat, y, reduction="sum")

        logs = {"loss": loss}

        return {"loss": loss, "log": logs, "progress_bar": logs}


class Discriminator(LightningModule):
    """A CNN for text classification
    architecture: Embedding >> Convolution >> Max-pooling >> Softmax
    """
    def __init__(self, num_classes, vocab_size, emb_dim, filter_sizes,
                 num_filters, dropout):
        super(Discriminator, self).__init__()
        self.emb = nn.Embedding(vocab_size, emb_dim)
        self.convs = nn.ModuleList([
            nn.Conv2d(1, n, (f, emb_dim))
            for (n, f) in zip(num_filters, filter_sizes)
        ])
        self.highway = nn.Linear(sum(num_filters), sum(num_filters))
        self.dropout = nn.Dropout(p=dropout)
        self.lin = nn.Linear(sum(num_filters), num_classes)
        self.softmax = nn.LogSoftmax()
        self.init_parameters()

    def forward(self, x):
        """
        Args:
            x: (batch_size * seq_len)
        """
        emb = self.emb(x).unsqueeze(1)  # batch_size * 1 * seq_len * emb_dim
        convs = [F.relu(conv(emb)).squeeze(3)
                 for conv in self.convs]  # [batch_size * num_filter * length]
        pools = [
            F.max_pool1d(conv, conv.size(2)).squeeze(2) for conv in convs
        ]  # [batch_size * num_filter]
        pred = torch.cat(pools, 1)  # batch_size * num_filters_sum
        highway = self.highway(pred)
        pred = (torch.sigmoid(highway) * F.relu(highway) +
                (1.0 - torch.sigmoid(highway)) * pred)
        pred = self.softmax(self.lin(self.dropout(pred)))
        return pred

    def init_parameters(self):
        for param in self.parameters():
            param.data.uniform_(-0.05, 0.05)

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self.forward(x)
        loss = F.nll_loss(y_hat, y, reduction="sum")

        logs = {"loss": loss}

        return {"loss": loss, "log": logs, "progress_bar": logs}


class Oracle(nn.Module):
    """Oracle"""
    def __init__(self, num_emb, emb_dim, hidden_dim, device):
        super(Oracle, self).__init__()
        self.num_emb = num_emb
        self.emb_dim = emb_dim
        self.hidden_dim = hidden_dim
        self.device = device
        self.emb = nn.Embedding(num_emb, emb_dim)
        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)
        self.lin = nn.Linear(hidden_dim, num_emb)
        self.softmax = nn.LogSoftmax()
        self.init_params()

    def forward(self, x):
        """
        Args:
            x: (batch_size, seq_len), sequence of tokens generated by generator
        """
        emb = self.emb(x)
        h0, c0 = self.init_hidden(x.size(0))
        output, (h, c) = self.lstm(emb, (h0, c0))
        pred = self.softmax(
            self.lin(output.contiguous().view(-1, self.hidden_dim)))
        return pred

    def step(self, x, h, c):
        """
        Args:
            x: (batch_size,  1), sequence of tokens generated by generator
            h: (1, batch_size, hidden_dim), lstm hidden state
            c: (1, batch_size, hidden_dim), lstm cell state
        """
        emb = self.emb(x)
        output, (h, c) = self.lstm(emb, (h, c))
        pred = F.softmax(self.lin(output.view(-1, self.hidden_dim)), dim=1)
        return pred, h, c

    def init_hidden(self, batch_size):
        h = Variable(
            torch.zeros((1, batch_size, self.hidden_dim), device=self.device))
        c = Variable(
            torch.zeros((1, batch_size, self.hidden_dim), device=self.device))
        return h, c

    def init_params(self):
        for param in self.parameters():
            param.data.normal_(0, 1)

    def sample(self, batch_size, seq_len):
        with torch.no_grad():
            x = Variable(
                torch.zeros((batch_size, 1), device=self.device).long())
            h, c = self.init_hidden(batch_size)
            samples = []
            for i in range(seq_len):
                output, h, c = self.step(x, h, c)
                x = output.multinomial(1)
                samples.append(x)
            output = torch.cat(samples, dim=1)
            return output


class SeqGAN(LightningModule):
    def __init__(self, hparams, device):
        super().__init__()
        self.hparams = hparams
        self.device = device

        self.generator = Generator(hparams.vocab_size, hparams.gen_emb_dim,
                                   hparams.gen_hid_dim, device)
        self.discriminator = Discriminator(
            2,
            hparams.vocab_size,
            hparams.dis_emb_dim,
            hparams.filter_sizes,
            hparams.num_filters,
            hparams.dropout_p,
        )

        self.oracle = Oracle(hparams.vocab_size, hparams.gen_emb_dim,
                             hparams.gen_hid_dim, device)

        self.rollout = Rollout(self.generator, 0.8)

    def forward(self, inputs):
        return self.generator(inputs)

    def adversarial_loss(self, prob, target, reward):
        n = target.size(0)
        c = prob.size(1)
        one_hot = torch.zeros((n, c), device=self.device)
        one_hot.scatter_(1, target.data.view((-1, 1)), 1)
        one_hot = one_hot.byte()
        one_hot = Variable(one_hot)
        loss = torch.masked_select(prob, one_hot)
        loss = loss * reward
        loss = -torch.sum(loss)
        return loss

    def training_step(self, batch, batch_idx, optimizer_idx):
        x, y = batch

        self.rollout.update_params()
        samples = self.generator.sample(x.shape[0], x.shape[1])

        if optimizer_idx == 0:
            zeros = torch.zeros((x.shape[0], 1)).long()

            inputs = torch.cat([zeros, samples], dim=1)[:, :-1].contiguous()
            targets = samples.contiguous().view(-1)
            rewards = (torch.tensor(
                self.rollout.get_reward(
                    samples, 16,
                    self.discriminator)).exp().view(-1)).to(self.device)

            prob = self.generator(inputs)
            loss = self.adversarial_loss(prob, targets, rewards)

            logs = {"loss": loss}

            return {"loss": loss, "log": logs, "progress_bar": logs}
        else:
            x, y = batch
            y_hat = self.forward(x)
            loss = F.nll_loss(y_hat, y, reduction="sum")

            logs = {"loss": loss}

            return {"loss": loss, "log": logs, "progress_bar": logs}

    def on_train_start(self):
        self.logger.log_hyperparams_metrics(
            self.hparams,
            {
                "train_loss": 100,
                "train_acc": 0,
            },
        )

    def configure_optimizers(self):
        return (
            [
                optim.Adam(self.generator.parameters(), lr=self.hparams.lr),
                optim.Adam(self.discriminator.parameters(),
                           lr=self.hparams.lr),
            ],
            [],
        )
